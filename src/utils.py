from pydantic import ValidationError
import re
import json
from typing import Optional, Dict, List
from pydantic import BaseModel, Field
from enum import Enum
from pathlib import Path
from cache import cached_llm_invoke

class MigrationAction(str, Enum):
    PENDING = "pending"
    CONTINUE = "continue"
    ESCALATE = "escalate"
    COMPLETE = "complete"
    RETRY = "retry"


class MigrationState(BaseModel):
    repo_structure: Dict[str, Dict] = Field(
        description="Directory structure of V1, V2, and target repos"
    )
    migrations: Dict[str, Dict] = Field(
        default_factory=dict,
        description="Component migration rules and transformations",
    )
    v1_components: Dict[str, Dict] = Field(
        default_factory=dict, description="V1 component details"
    )
    v2_components: Dict[str, Dict] = Field(
        default_factory=dict, description="V2 component details"
    )
    component_map:  Optional[Dict[str, Dict]] = Field(
        default_factory=dict, description="Mapping between old and new components"
    )
    constraints: List[Dict] = Field(
        default_factory=list,
        description="Automatically identified migration constraints",
    )
    verification_rules: List[Dict] = Field(
        default_factory=list, description="Validation rules generated by AI"
    )
    migration_plan: List[Dict] = Field(
        default_factory=list, description="Step-by-step migration instructions"
    )
    current_file: Optional[str] = Field(
        None, description="File currently being processed"
    )
    modified_code: Dict[str, str] = Field(
        default_factory=dict, description="Migrated code versions"
    )
    action: MigrationAction = Field(
        MigrationAction.PENDING, description="Current workflow action state"
    )
    verification_errors: List[Dict] = Field(
        default_factory=list, description="Errors found during verification"
    )
    retry_count: int = Field(0, description="Number of retry attempts")
    human_feedback: Optional[Dict] = Field(
        None, description="Storage for human input responses"
    )


def needs_human_approval(state: MigrationState) -> bool:
    if any(
        e.get("category") == "accessibility" and e.get("severity") == "critical"
        for e in state.verification_errors
    ):
        return True
    if state.retry_count >= 3:
        return True
    if state.action == MigrationAction.ESCALATE:
        return True
    if any(step.get("requires_human_approval", False) for step in state.migration_plan):
        return True
    return False


async def execute_workflow(workflow, initial_state: MigrationState):
    state = initial_state
    MAX_RETRIES = 3
    while True:
        state = await workflow.invoke(state)
        if needs_human_approval(state):
            print("\n--- Human Approval Required ---")
            print(f"Reason: {get_escalation_reason(state)}")
            state = await get_human_feedback(state)
            if state.human_feedback.get("decision") == "abort":
                raise MigrationAbortedError()
        if state.action == MigrationAction.COMPLETE:
            if validate_final_state(state):
                return state
            else:
                state.action = MigrationAction.ESCALATE
        if state.action == MigrationAction.RETRY:
            if state.retry_count < MAX_RETRIES:
                state.retry_count += 1
                state.action = MigrationAction.CONTINUE
            else:
                state.action = MigrationAction.ESCALATE


def get_escalation_reason(state: MigrationState) -> str:
    if state.retry_count >= 3:
        return "Maximum retry attempts exceeded"
    if any(e["category"] == "accessibility" for e in state.verification_errors):
        return "Critical accessibility issues found"
    return "Unresolved migration conflicts"


def validate_final_state(state: MigrationState) -> bool:
    return all(
        [
            len(state.verification_errors) == 0,
            len(state.migration_plan) > 0,
            any(state.modified_code.values()),
        ]
    )


class MigrationAbortedError(Exception):
    pass


def handle_errors(state: MigrationState) -> MigrationState:
    error_details = state.verification_errors
    categorized = {"critical": [], "warning": [], "info": []}
    for error in error_details:
        category = error.get("category", "").lower()
        if category == "accessibility":
            categorized["critical"].append(error)
        elif category == "styling":
            categorized["warning"].append(error)
        else:
            categorized["info"].append(error)
    if categorized["critical"]:
        action = MigrationAction.ESCALATE
    elif len(categorized["warning"]) > 2:
        action = MigrationAction.RETRY
    else:
        action = MigrationAction.CONTINUE
    return state.copy(update={"action": action, "retry_count": state.retry_count + 1})


async def get_human_feedback(state: MigrationState) -> MigrationState:
    print("\nHuman Intervention Required:")
    print(f"File: {state.current_file}")
    print("Issues:")
    for error in state.verification_errors:
        print(f"- {error}")
    feedback = input("Enter resolution (override/skip/abort): ").strip().lower()
    return state.copy(
        update={
            "human_feedback": {"decision": feedback},
            "action": (
                MigrationAction.CONTINUE
                if feedback == "override"
                else MigrationAction.ESCALATE
            ),
        }
    )


def parse_constraints(llm_output: str) -> List[Dict]:
    try:
        return json.loads(llm_output)
    except json.JSONDecodeError:
        pass
    constraints = []
    pattern = r"\d+\.\s(.+?):\s(.+)"
    matches = re.findall(pattern, llm_output)
    for match in matches:
        constraints.append(
            {
                "type": match[0].strip(),
                "description": match[1].strip(),
                "severity": "high" if "breaking" in match[0].lower() else "medium",
            }
        )
    return constraints


def parse_plan(llm_output: str) -> list[dict[str, str]]:
    plan = []
    try:
        # Parse the JSON string into a dictionary
        llm_output_dict = json.loads(llm_output)
        
        # Get the migration_plan list from the dictionary
        migration_plan = llm_output_dict.get("migration_plan", [])
        
        for item in migration_plan:
            if "Step" in item:  # To identify step actions
                plan.append(
                    {
                        "action": item.strip(),
                        "status": "pending",
                        "type": "step",
                    }
                )
            # Add further conditions here if necessary for other types of actions
            
    except Exception as e:
        print(f"Error parsing plan: {e}")
    
    print("Parsed Plan:", plan)
    return plan if plan else []

def parse_rules(llm_output: str) -> List[Dict[str, str]]:
    print("parse_rules called", llm_output)
    
    try:
        # Attempt to parse the input as JSON directly
        parsed = json.loads(llm_output)
        
        # If parsed is a list of rules, return it directly
        if isinstance(parsed, list):
            return parsed
        
        # If parsed is a dictionary and contains 'verification_rules', return that
        if isinstance(parsed, dict) and "verification_rules" in parsed:
            return parsed["verification_rules"]
        
        # If it's a dictionary without 'verification_rules', wrap it in a list and return
        elif isinstance(parsed, dict):
            return [parsed]
    except Exception as e:
        print(f"JSON parsing failed: {e}")
    
    # Fallback logic if no JSON format was detected
    rules = []
    current_rule = None
    for line in llm_output.split("\n"):
        line = line.strip()
        if not line:
            continue
        if line.startswith("- **Rule**: "):
            if current_rule:
                rules.append(current_rule)
            current_rule = {
                "rule": line[11:].strip(),
                "status": "pending",
                "details": [],
            }
        elif line.startswith("  -") and current_rule:
            current_rule["details"].append(line[3:].strip())
    
    if current_rule:
        rules.append(current_rule)
    
    return rules

def read_file(path):
    print("read_file called")
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def save_state(state: MigrationState, file_path: str):
    with open(file_path, "w") as f:
        json.dump(state.dict(), f, indent=2)


def load_state(file_path: str) -> Optional[MigrationState]:
    if Path(file_path).exists():
        with open(file_path, "r") as f:
            if Path(file_path).stat().st_size == 0:
                return None
            state_data = json.load(f)
        return MigrationState(**state_data)
    return None


def validate_migration(state: MigrationState) -> bool:
    errors = []

    # Check component migrations
    for old_comp, rule in state.migrations.items():
        if old_comp == "html_mappings":
            continue

        new_tag = rule["new_tag"]
        for file_path, content in state.modified_code.items():
            # Check old component presence
            if f"<{old_comp}" in content.lower():
                errors.append(f"Found unmigrated {old_comp} in {file_path}")

            # Check required attributes
            if new_tag in content:
                for req_attr in rule.get("required", {}):
                    if f"{req_attr}=" not in content:
                        errors.append(
                            f"Missing required attribute {req_attr} for {new_tag} in {file_path}"
                        )

    # Check HTML analogs
    for html_tag, wc_comp in state.migrations.get("html_mappings", {}).items():
        for file_path, content in state.modified_code.items():
            if f"<{html_tag}" in content.lower():
                errors.append(f"Found unmigrated HTML analog {html_tag} in {file_path}")

    if errors:
        print("\nValidation Errors:")
        for error in errors:
            print(f"  - {error}")
        return False
    return True


def load_component_docs(repo_structure: Dict, version: str = "v1") -> Dict[str, str]:
    """
    Load component documentation from a repository.

    For v1:
      - Documentation is located at:
        <repo_root>/stencil-workspace/storybook/stories/components
      - Expects markdown (.md, .txt) files.

    For v2:
      - For each component folder in the repo structure, documentation is in a file named 'stories.ts'.

    Returns:
      A dictionary mapping component names (lowercase) to their documentation content.
    """
    docs = {}
    root = Path(repo_structure.get("root", ""))
    structure = repo_structure.get("structure", {})
    print("root", version)
    if version.lower() == "v1":
        docs_path = root / "stencil-workspace\\storybook\\stories\\components\\"
        if docs_path.exists():
            for folder in docs_path.glob("*"):
                for file in folder.glob("*"):
                    # Assume the file name (without extension) matches the component name.
                    component_name = (
                        file.stem.lower()
                        .replace("-storybook-docs", "")
                        .replace(".stories", "")
                    )
                    try:
                        with file.open("r", encoding="utf-8") as f:
                            content = f.read()

                            docs[component_name] = content

                    except Exception as e:
                        print(f"Error reading doc file {file}: {e}")
    elif version.lower() == "v2":

        comp_dir = root / "src\\components\\"
        if comp_dir.exists():
            for folder in comp_dir.glob("*"):
                for stories_file in folder.glob("*.stories.ts"):
                    # Assume that if a 'stories.ts' file exists, then the directory name is the component name.
                    if stories_file.exists():
                        component_name = folder.name
                        try:
                            with stories_file.open("r", encoding="utf-8") as f:
                                docs[component_name] = f.read()
                        except Exception as e:
                            print(f"Error reading {stories_file}: {e}")
    return docs

def get_event_context(content: str, event: str) -> str:
    prompt = f"""
Given the following component code snippet:
--------------------------
{content}
--------------------------
Explain the purpose and usage of the event '{event}' in this component. 
Provide a concise comment.
"""
    response = cached_llm_invoke(prompt)
    return response.content.strip()

def get_slot_context(content: str, slot: str) -> str:
    prompt = f"""
Given the following component code snippet:
--------------------------
{content}
--------------------------
Explain the purpose and usage of the slot '{slot}' in this component. 
Provide a concise comment.
"""
    response = cached_llm_invoke(prompt)
    return response.content.strip()

def get_prop_context(content: str, prop: str) -> str:
    prompt = f"""
    Given the following component code snippet:
    --------------------------
    {content}
    --------------------------
    Explain the purpose and usage of the prop '{prop}' in this component.
    Provide a concise comment.
    """
    response = cached_llm_invoke(prompt)
    return response.content.strip()

def parse_component_file(path: Path) -> Dict:
    """Parse a component file to extract props, events, and slots."""
    with path.open("r", encoding="utf-8") as f:
        content = f.read()
    
    props = re.findall(r"@Prop\(\)\s+(\w+):", content)
    events = re.findall(r"@Event\(\)\s+(\w+):", content)
    slots = re.findall(r'<slot name="(\w+)"', content)

    return {
        "props": [{
            "name": prop,
            "comment": get_prop_context(content, prop)
        } for prop in props],
        "events": [{
            "name": event,
            "comment": get_event_context(content, event)
        } for event in events],
        "slots": [{
            "name": slot,
            "comment": get_slot_context(content, slot)
        } for slot in slots],
    }

def extract_component_details(repo_structure: Dict, version: str = "v1") -> Dict:
    """
    Extract component details from repository structure.

    Reads each component file and extracts properties, events, and slots.
    Also loads documentation based on the repo version and adds it to the details.
    """
    components = {}
    print("extract_component_details called", version)
    docs = load_component_docs(repo_structure, version)

    for rel_path, details in repo_structure.get("structure", {}).items():
        for component in details.get("components", []):

            component_file = Path(repo_structure["root"]) / rel_path / f"{component}"
            if component_file.exists():
                parsed = parse_component_file(component_file)
                # Lookup documentation using a lowercase key (you may adjust how you match names)
                doc = docs.get(component.strip(".tsx"), "")
                parsed["documentation"] = doc
                components[component] = parsed
    return components


def parse_migration_rules(llm_output: str) -> dict:
    """Parse LLM migration rules response into structured mapping."""
    print("\nparse_migration_rules called\n", llm_output)

    try:
        # Remove possible Markdown code block formatting (```json ... ```)
        cleaned_output = llm_output.strip("```json\n").strip("```").strip()

        # Extract JSON using regex (handling multiple `{}` pairs)
        json_match = re.search(r"\{.*\}", cleaned_output, re.DOTALL)

        if not json_match:
            print("⚠️ No valid JSON found in LLM response.")
            return {"migrations": {}, "examples": {}}

        json_str = json_match.group(0).strip()  # Extract JSON part
        parsed = json.loads(json_str)  # Parse JSON

        # Debug extracted JSON
        print("\n✅ Parsed JSON:\n", json.dumps(parsed, indent=2))

        # Ensure keys exist
        return {
            "migrations": parsed.get("migrations", parsed.get("mappings", {})),  # Handle both possible key names
            "examples": parsed.get("html_mappings", {}),
        }
    except json.JSONDecodeError as e:
        print(f"🚨 JSON Parsing Error: {e}")
        return {"migrations": {}, "examples": {}}
    except Exception as e:
        print(f"❌ Unexpected Error: {e}")
        return {"migrations": {}, "examples": {}}
def read_file(path):
    print("read_file called")
    with open(path, "r") as f:
        return f.read()


# Collect all files from the target directory (or directories) in the repo structure.
def get_all_files_from_structure(
    structure: Dict[str, Dict], repo_root: str, extensions: List[str]
) -> List[str]:
    all_files = [
        "/tmp/GitHub-Issues-Extraction.git/TDAAS-Web/src/app/prompt2-code/prompt2-code.component.html"
    ]
    # allowed_extensions = extensions  # Allowed file extensions

    # for rel_path, details in structure.items():
    #     # Build the absolute directory path
    #     dir_path = os.path.join(repo_root, rel_path)

    #     # Process only if the directory exists
    #     if os.path.isdir(dir_path):
    #         for file in details.get("files", []):
    #             file_path = os.path.join(dir_path, file)

    #             # Check if it's a valid file and has an allowed extension
    #             if (
    #                 os.path.isfile(file_path)
    #                 and os.path.splitext(file)[1] in allowed_extensions
    #             ):
    #                 all_files.append(file_path)

    return all_files


def apply_migration_rules(code: str, state: MigrationState) -> str:
    """Safer version with validation checks"""
    original = code

    try:
        # 1. Replace HTML analogs first
        for html_tag, wc_comp in state.migrations.get("html_mappings", {}).items():
            code = re.sub(
                rf"<{html_tag}\b([^>]*)>",
                lambda m: f"<{wc_comp} {m.group(1)}>",
                code,
                flags=re.IGNORECASE,
            )
            code = re.sub(rf"</{html_tag}>", f"</{wc_comp}>", code, flags=re.IGNORECASE)

        # 2. Replace Modus 1.0 components
        for old_comp, rule in state.migrations.items():
            if old_comp == "html_mappings":
                continue

            # Tag replacement
            code = re.sub(
                rf"<{old_comp}\b([^>]*)>",
                lambda m: process_component_tag(m, rule),
                code,
                flags=re.IGNORECASE,
            )
            code = re.sub(
                rf"</{old_comp}>", f'</{rule["new_tag"]}>', code, flags=re.IGNORECASE
            )

        return code
    except Exception as e:
        print(f"Rolling back changes: {str(e)}")
        return original


def process_component_tag(match, rule):
    """Process individual component tags with attribute mapping"""
    attrs = match.group(1)
    new_tag = rule["new_tag"]

    # Replace attributes
    for old_attr, new_attr in rule.get("props", {}).items():
        attrs = re.sub(
            rf'\b{old_attr}=["\']', f'{new_attr}="', attrs, flags=re.IGNORECASE
        )

    # Add required attributes
    for attr, value in rule.get("required", {}).items():
        if f"{attr}=" not in attrs:
            attrs = f'{attr}="{value}" {attrs}'

    return f"<{new_tag} {attrs.strip()}>"


def validate_prop(match, new_prop, v2_spec):
    """Validate property values against v2 specs"""
    value = match.group(1)
    valid_values = v2_spec.get("props", {}).get(new_prop, {}).get("values", [])

    if valid_values and value not in valid_values:
        return f'{new_prop}="{valid_values[0]}"  # Original: {value}'
    return f'{new_prop}="{value}"'


def validate_html_analog(
    code: str, html_tag: str, wc_component: str, state: MigrationState
) -> str:
    """
    Validate and convert HTML elements to Web Components with attribute validation
    Returns modified code and validation warnings
    """
    pattern = re.compile(rf"<{html_tag}\b(?P<attrs>[^>]*)>", re.IGNORECASE | re.DOTALL)
    wc_spec = state.v2_specs.get(wc_component)

    if not wc_spec:
        print(f"Warning: No spec found for {wc_component}")
        return code

    for match in reversed(list(pattern.finditer(code))):  # Reverse to maintain offsets
        full_match = match.group(0)
        attrs_str = match.group("attrs")

        # Parse attributes with Angular binding support
        attrs = parse_attributes(attrs_str)
        validated_attrs = {}
        warnings = []

        # Validate each attribute
        for attr, value in attrs.items():
            # Handle Angular bindings
            is_bound = attr.startswith(("[", "(")) or attr.startswith("bind-")
            clean_attr = (
                attr.replace("[", "").replace("]", "").replace("(", "").replace(")", "")
            )

            if clean_attr in wc_spec.props:
                # Valid attribute with binding syntax
                validated_attrs[attr] = value
            elif clean_attr in state.html_analog_map.values():
                # Map HTML native attributes to WC props
                wc_prop = next(
                    (k for k, v in state.html_analog_map.items() if v == clean_attr),
                    None,
                )
                if wc_prop and wc_prop in wc_spec.props:
                    new_attr = f"[{wc_prop}]" if is_bound else wc_prop
                    validated_attrs[new_attr] = value
            else:
                warnings.append(f"Removed invalid attribute: {attr}")

        # Add required attributes
        for prop, details in wc_spec.prop_details.items():
            if details.get("required") and prop not in validated_attrs:
                if details.get("default"):
                    validated_attrs[prop] = details["default"]
                    warnings.append(
                        f"Added required attribute {prop}={details['default']}"
                    )
                else:
                    warnings.append(f"Missing required attribute: {prop}")

        # Build new tag
        new_attrs = " ".join(
            [format_attribute(k, v) for k, v in validated_attrs.items()]
        )
        new_tag = f"<{wc_component} {new_attrs}>"

        # Preserve closing style
        if "/>" in full_match:
            new_tag = new_tag.replace(">", "/>")

        # Update code and log warnings
        code = code[: match.start()] + new_tag + code[match.end() :]
        if warnings:
            print(f"Validation warnings for {html_tag} -> {wc_component}:")
            print("\n".join(f"  - {w}" for w in warnings))

    return code


def parse_attributes(attrs_str: str) -> Dict[str, str]:
    """Parse HTML attributes with Angular binding support"""
    attrs = {}
    # Match Angular bindings and regular attributes
    attr_pattern = re.compile(
        r'(?P<name>\[?[\w-]+\]?|\(?[\w-]+\)?|bind-[\w-]+)\s*=\s*(?P<value>"[^"]*"|\'[^\']*\'|{[^}]*})',
        re.IGNORECASE,
    )
    for match in attr_pattern.finditer(attrs_str):
        name = match.group("name").strip()
        value = match.group("value").strip()
        attrs[name] = value
    return attrs


def format_attribute(name: str, value: str) -> str:
    """Format attribute for Angular/WC compatibility"""
    # Handle boolean attributes
    if value.lower() in ("true", "false"):
        return f'[{name}]="{value}"'

    # Preserve binding syntax
    if name.startswith(("[", "(")):
        return f'{name}="{value}"'

    # Standard attribute
    return f'{name}="{value}"'
